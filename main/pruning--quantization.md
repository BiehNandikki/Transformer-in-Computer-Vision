### Pruning & Quantization
- (arXiv 2021.04) Visual Transformer Pruning, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)
- (arXiv 2021.06) Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)
- (arXiv 2021.11) PTQ4ViT: Post-Training Quantization Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf), [[Code]](https://github.com/hahnyuan/PTQ4ViT)
- (arXiv 2021.11) FQ-ViT: Fully Quantized Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)
- (arXiv 2022.01) Q-ViT: Fully Differentiable Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)
- (arXiv 2022.01) VAQF: Fully Automatic Software-hardware Co-design Framework for Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)
- (arXiv 2022.03) Patch Similarity Aware Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.02250.pdf)
- (arXiv 2022.03) CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction, [[Paper]](https://arxiv.org/pdf/2203.04570.pdf)
- (arXiv 2022.07) I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2207.01405.pdf)
- (arXiv 2022.08) Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization, [[Paper]](https://arxiv.org/pdf/2208.05163.pdf)
- (arXiv 2022.09) PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.05687.pdf), [[Code]](https://github.com/zkkli/PSAQ-ViT)
- (arXiv 2022.10) EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, [[Paper]](https://arxiv.org/pdf/2210.00181.pdf)
- (arXiv 2022.10) SaiT: Sparse Vision Transformers through Adaptive Token Pruning, [[Paper]](https://arxiv.org/pdf/2210.05832.pdf)
- (arXiv 2022.10) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.06707.pdf), [[Code]](https://github.com/YanjingLi0202/Q-ViT)
- (arXiv 2022.10) oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.09223.pdf)
- (arXiv 2022.11) CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.09643.pdf)
- (arXiv 2022.11) NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.16056.pdf)
- (arXiv 2022.12) Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis, [[Paper]](https://arxiv.org/pdf/2212.03185.pdf), [[Code]](https://github.com/TencentARC/BasicVQ-GEN)
- (arXiv 2022.12) RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08254.pdf)
- (arXiv 2023.02) Oscillation-free Quantization for Low-bit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.02210.pdf)
- (arXiv 2023.03) Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction, [[Paper]](https://arxiv.org/pdf/2303.12557.pdf), [[Code]](https://github.com/Q-HyViT)
- (arXiv 2023.03) Scaled Quantization for the Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.13601.pdf)
- (arXiv 2023.03) Towards Accurate Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.14341.pdf)
- (arXiv 2023.04) Q-DETR: An Efficient Low-Bit Quantized Detection Transformer, [[Paper]](https://arxiv.org/pdf/2304.00253.pdf)
- (arXiv 2023.04) Attention Map Guided Transformer Pruning for Edge Device, [[Paper]](https://arxiv.org/pdf/2304.01452.pdf)
- (arXiv 2023.05) Patch-wise Mixed-Precision Quantization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.06559.pdf)
- (arXiv 2023.05) Boost Vision Transformer with GPU-Friendly Sparsity and Quantization, [[Paper]](https://arxiv.org/pdf/2305.10727.pdf)
- (arXiv 2023.05) Bi-ViT: Pushing the Limit of Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2305.12354.pdf)
- (arXiv 2023.07) Variation-aware Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2307.00331.pdf), [[Code]](https://github.com/HuangOwen/VVTQ)
- (arXiv 2023.08) Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2308.10814.pdf), [[Code]](https://github.com/enyac-group/evol-q)
- (arXiv 2023.08) Vision Transformer Pruning Via Matrix Decomposition,  [[Paper]](https://arxiv.org/pdf/2308.10839.pdf)
- (arXiv 2023.09) Transformer-VQ: Linear-Time Transformers via Vector Quantization,  [[Paper]](https://arxiv.org/pdf/2309.16354.pdf), [[Code]](https://github.com/transformer-vq/transformer_vq)
- (arXiv 2023.10) LLM-FP4: 4-Bit Floating-Point Quantized Transformers, [[Paper]](https://arxiv.org/pdf/2310.16836.pdf), [[Code]](https://github.com/nbasyl/LLM-FP4)
- (arXiv 2023.12) QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02220.pdf)
- (arXiv 2024.01) LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation, [[Paper]](https://arxiv.org/pdf/2401.11243.pdf)
- (arXiv 2024.01) MPTQ-ViT: Mixed-Precision Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.14895.pdf)
- (arXiv 2024.03) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning, [[Paper]](https://arxiv.org/pdf/2403.14047.pdf)
- (arXiv 2024.04) Instance-Aware Group Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.00928.pdf), [[Code]](https://cvlab.yonsei.ac.kr/projects/IGQ-ViT/)
- (arXiv 2024.04) Data-independent Module-aware Pruning for Hierarchical Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.13648.pdf), [[Code]](https://github.com/he-y/Data-independent-Module-Aware-Pruning)
- (arXiv 2024.05) Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey, [[Paper]](https://arxiv.org/pdf/2405.00314.pdf), [[Code]](https://github.com/DD-DuDa/awesome-vit-quantization-acceleration)
- (arXiv 2024.05) Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2405.03882.pdf)
- (arXiv 2024.05) MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization, [[Paper]](https://arxiv.org/pdf/2405.17873.pdf)
- (arXiv 2024.06) ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation, [[Paper]](https://arxiv.org/pdf/2406.02540.pdf), [[Code]](https://github.com/A-suozhang/ViDiT-Q)
- (arXiv 2024.06) MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction, [[Paper]](https://arxiv.org/pdf/2406.09229.pdf)
- (arXiv 2024.06) An Analysis on Quantizing Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.11100.pdf)
- (arXiv 2024.06) ViT-1.58b: Mobile Vision Transformers in the 1-bit Era, [[Paper]](https://arxiv.org/pdf/2406.18051)
- (arXiv 2024.06) Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.17343), [[Code]](https://github.com/Juanerx/Q-DiT)
- (arXiv 2024.07) ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.02763)
